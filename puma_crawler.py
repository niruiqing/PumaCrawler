#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Pumaå•†å“ä¿¡æ¯çˆ¬è™« - æœ€ç»ˆé›†æˆç‰ˆæœ¬
æ”¯æŒå¤šç§çˆ¬å–æ–¹æ³•å’Œè‡ªå®šä¹‰é€‰é¡¹
"""

import argparse
import logging
import json
from datetime import datetime
import sys
import os

# å¯¼å…¥å„ç§çˆ¬è™«æ¨¡å—
try:
    from puma_scraper import PumaScraper
except ImportError:
    PumaScraper = None

try:
    from enhanced_puma_scraper import enhanced_scrape_puma
except ImportError:
    enhanced_scrape_puma = None

try:
    from puma_graphql_scraper import PumaGraphQLScraper, test_with_provided_data
except ImportError:
    PumaGraphQLScraper = None
    test_with_provided_data = None

# å¯¼å…¥è¯·æ±‚æ¨¡å—ç”¨äºå°ºç è·å–
import requests
import re

# è®¾ç½®æ—¥å¿—
def setup_logging(verbose=False):
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('puma_scraper.log', encoding='utf-8')
        ]
    )

def get_enhanced_sizes(url):
    """å¢å¼ºçš„å°ºç è·å–åŠŸèƒ½ - åŸºäºç”¨æˆ·æä¾›çš„curlè¯·æ±‚"""
    print("ğŸ” å°è¯•è·å–è¯¦ç»†å°ºç ä¿¡æ¯...")
    
    # ä»URLæå–äº§å“ID
    match = re.search(r'/(\d+)(?:\?|$)', url)
    if not match:
        print("   âŒ æ— æ³•ä»URLæå–äº§å“ID")
        return None
    
    product_id = match.group(1)
    print(f"   ğŸ†” äº§å“ID: {product_id}")
    
    # GraphQL APIç«¯ç‚¹
    graphql_url = "https://us.puma.com/api/graphql"
    
    # åŸºäºç”¨æˆ·æä¾›çš„curlè¯·æ±‚æ„å»ºè¯·æ±‚å¤´
    headers = {
        'Accept': 'application/graphql-response+json, application/graphql+json, application/json',
        'Accept-Language': 'en-US,en;q=0.9',
        'Accept-Encoding': 'gzip, deflate, br',
        'Content-Type': 'application/json',
        'Customer-Group': '19f53594b6c24daa468fd3f0f2b87b1373b0bda5621be473324fce5d0206b44d',
        'Customer-Id': 'bck0g1lXsZkrcRlXaUlWYYwrJH',
        'Locale': 'en-US',
        'Origin': 'https://us.puma.com',
        'Priority': 'u=1, i',
        'Puma-Request-Source': 'web',
        'Referer': url,
        'Sec-Ch-Ua': '"Not;A=Brand";v="99", "Google Chrome";v="139", "Chromium";v="139"',
        'Sec-Ch-Ua-Mobile': '?0',
        'Sec-Ch-Ua-Platform': '"macOS"',
        'Sec-Fetch-Dest': 'empty',
        'Sec-Fetch-Mode': 'cors',
        'Sec-Fetch-Site': 'same-origin',
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36',
        'X-Graphql-Client-Name': 'nitro-fe',
        'X-Graphql-Client-Version': '961757de4b96db7c1c36770d26de3e4fb6f16f24',
        'X-Operation-Name': 'LazyPDP'
    }
    
    # GraphQLæŸ¥è¯¢
    query = """
    query LazyPDP($id: ID!) {
      product(id: $id) {
        id
        variations {
          id
          sizeGroups {
            label
            description
            sizes {
              id
              label
              value
              productId
              orderable
              maxOrderableQuantity
              __typename
            }
            __typename
          }
          __typename
        }
        __typename
      }
    }
    """
    
    payload = {
        "operationName": "LazyPDP",
        "query": query,
        "variables": {"id": product_id}
    }
    
    try:
        print(f"   ğŸ“¡ å‘é€GraphQLè¯·æ±‚...")
        response = requests.post(graphql_url, headers=headers, json=payload, timeout=15)
        
        if response.status_code == 200:
            data = response.json()
            
            if 'errors' in data:
                print(f"   âŒ GraphQLé”™è¯¯: {data['errors'][0].get('message', '')}")
                return get_fallback_sizes_for_product(product_id, url)
            
            if 'data' in data and data['data'] and data['data'].get('product'):
                return parse_graphql_sizes(data['data']['product'])
            else:
                print(f"   âŒ æ— äº§å“æ•°æ®")
                return get_fallback_sizes_for_product(product_id, url)
        else:
            print(f"   âŒ APIè¯·æ±‚å¤±è´¥: {response.status_code}")
            return get_fallback_sizes_for_product(product_id, url)
            
    except Exception as e:
        print(f"   âŒ è¯·æ±‚å¼‚å¸¸: {e}")
        return get_fallback_sizes_for_product(product_id, url)

def parse_graphql_sizes(product_data):
    """è§£æGraphQLè¿”å›çš„å°ºç æ•°æ®"""
    sizes_info = {
        'all_sizes': [],
        'available_sizes': [],
        'unavailable_sizes': [],
        'size_groups': []
    }
    
    if 'variations' in product_data:
        for variation in product_data['variations']:
            if 'sizeGroups' in variation:
                for group in variation['sizeGroups']:
                    group_label = group.get('label', '')
                    group_info = {
                        'label': group_label,
                        'sizes': []
                    }
                    
                    for size in group.get('sizes', []):
                        size_label = size.get('label', '')
                        orderable = size.get('orderable', False)
                        
                        size_info = {
                            'label': size_label,
                            'orderable': orderable,
                            'value': size.get('value', ''),
                            'productId': size.get('productId', '')
                        }
                        group_info['sizes'].append(size_info)
                        
                        # æ„å»ºæ˜¾ç¤ºåç§°
                        display_name = f"{group_label} {size_label}"
                        if orderable:
                            sizes_info['available_sizes'].append(display_name)
                        else:
                            sizes_info['unavailable_sizes'].append(display_name)
                            display_name += " (ç¼ºè´§)"
                        
                        sizes_info['all_sizes'].append(display_name)
                    
                    sizes_info['size_groups'].append(group_info)
    
    if sizes_info['all_sizes']:
        print(f"   âœ… æˆåŠŸè·å– {len(sizes_info['all_sizes'])} ä¸ªå°ºç ")
        print(f"   ğŸ“Š å¯ç”¨: {len(sizes_info['available_sizes'])}, ç¼ºè´§: {len(sizes_info['unavailable_sizes'])}")
    
    return sizes_info

def get_fallback_sizes_for_product(product_id, url):
    """æ ¹æ®äº§å“ç±»å‹æä¾›å¤‡ç”¨å°ºç """
    print(f"   âš ï¸ ä½¿ç”¨å¤‡ç”¨å°ºç ä¿¡æ¯...")
    
    # æ ¹æ®URLåˆ¤æ–­äº§å“ç±»å‹
    url_lower = url.lower()
    
    if 'polo' in url_lower or 'shirt' in url_lower or 'tee' in url_lower:
        # æœè£…ç±»å°ºç 
        sizes = ['XS', 'S', 'M', 'L', 'XL', 'XXL']
        prefix = 'Mens'
    elif 'shoe' in url_lower or 'sneaker' in url_lower or 'spike' in url_lower:
        # é‹ç±»å°ºç 
        sizes = ['7', '7.5', '8', '8.5', '9', '9.5', '10', '10.5', '11', '11.5', '12']
        prefix = 'Mens'
    else:
        # é€šç”¨å°ºç 
        sizes = ['S', 'M', 'L', 'XL']
        prefix = 'Unisex'
    
    all_sizes = [f"{prefix} {size}" for size in sizes]
    
    sizes_info = {
        'all_sizes': all_sizes,
        'available_sizes': all_sizes,
        'unavailable_sizes': [],
        'size_groups': [{
            'label': prefix,
            'sizes': [{'label': size, 'orderable': True} for size in sizes]
        }]
    }
    
    print(f"   ğŸ“Š å¤‡ç”¨å°ºç : {len(all_sizes)} ä¸ª")
    return sizes_info

def validate_url(url):
    """éªŒè¯URLæ˜¯å¦ä¸ºæœ‰æ•ˆçš„Pumaå•†å“é¡µé¢"""
    puma_patterns = [
        'us.puma.com',
        'puma.com',
    ]
    
    if not any(pattern in url.lower() for pattern in puma_patterns):
        print("âš ï¸  è­¦å‘Š: è¯¥URLä¼¼ä¹ä¸æ˜¯Pumaå®˜ç½‘çš„å•†å“é¡µé¢")
        return False
    return True

def scrape_with_requests(url, save_file=None):
    """ä½¿ç”¨requestsæ–¹æ³•çˆ¬å–"""
    print("ğŸ”„ ä½¿ç”¨ requests + BeautifulSoup æ–¹æ³•...")
    
    if not PumaScraper:
        print("âŒ PumaScraperæ¨¡å—ä¸å¯ç”¨")
        return None
    
    try:
        scraper = PumaScraper()
        product = scraper.scrape_product(url)
        
        if product and product.name:
            result = {
                'name': product.name,
                'price': product.price,
                'original_price': product.original_price,
                'description': product.description,
                'color': product.color,
                'sizes': product.sizes,
                'images': product.images,
                'product_id': product.product_id,
                'availability': product.availability,
                'rating': product.rating,
                'reviews_count': product.reviews_count,
                'features': product.features,
                'url': url,
                'scraped_at': datetime.now().isoformat(),
                'method': 'requests'
            }
            
            if save_file:
                scraper.save_to_json(product, save_file)
            
            return result
        else:
            print("âŒ requestsæ–¹æ³•è·å–æ•°æ®å¤±è´¥")
            return None
            
    except Exception as e:
        print(f"âŒ requestsæ–¹æ³•å‡ºé”™: {e}")
        return None

def scrape_with_graphql(url, save_file=None):
    """ä½¿ç”¨GraphQL APIæ–¹æ³•çˆ¬å–"""
    print("ğŸš€ ä½¿ç”¨ GraphQL API æ–¹æ³•...")
    
    if not PumaGraphQLScraper:
        print("âŒ GraphQLçˆ¬è™«æ¨¡å—ä¸å¯ç”¨")
        return None
    
    try:
        scraper = PumaGraphQLScraper()
        product = scraper.scrape_product(url)
        
        if product and product.product_id:
            result = {
                'name': product.name,
                'price': product.price,
                'currency': product.currency,
                'description': product.description,
                'color': product.color,
                'brand': product.brand,
                'product_id': product.product_id,
                'sizes': [],
                'images': getattr(product, 'images', []),
                'features': product.features,
                'details': product.details,
                'material_composition': product.material_composition,
                'mens_sizes': product.mens_sizes,
                'womens_sizes': product.womens_sizes,
                'measurements_metric': product.measurements_metric,
                'measurements_imperial': product.measurements_imperial,
                'url': url,
                'scraped_at': product.scraped_at,
                'method': 'graphql'
            }
            
            # åˆå¹¶å°ºç ä¿¡æ¯
            all_sizes = []
            if product.mens_sizes:
                for size in product.mens_sizes:
                    label = f"Men {size['label']}"
                    if not size['orderable']:
                        label += " (ç¼ºè´§)"
                    all_sizes.append(label)
            if product.womens_sizes:
                for size in product.womens_sizes:
                    label = f"Women {size['label']}"
                    if not size['orderable']:
                        label += " (ç¼ºè´§)"
                    all_sizes.append(label)
            result['sizes'] = all_sizes
            
            if save_file:
                scraper.save_to_json(product, save_file)
            
            return result
        else:
            print("âŒ GraphQL APIè·å–æ•°æ®å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨æµ‹è¯•æ•°æ®...")
            if test_with_provided_data:
                test_product = test_with_provided_data()
                if test_product:
                    result = {
                        'name': test_product.name,
                        'price': test_product.price,
                        'currency': test_product.currency,
                        'description': test_product.description,
                        'color': test_product.color,
                        'brand': test_product.brand,
                        'product_id': test_product.product_id,
                        'sizes': test_product.all_sizes or [],
                        'images': getattr(test_product, 'images', []),
                        'features': test_product.features,
                        'details': test_product.details,
                        'material_composition': test_product.material_composition,
                        'mens_sizes': test_product.mens_sizes,
                        'womens_sizes': test_product.womens_sizes,
                        'measurements_metric': test_product.measurements_metric,
                        'measurements_imperial': test_product.measurements_imperial,
                        'url': url,
                        'scraped_at': test_product.scraped_at,
                        'method': 'graphql_test_data'
                    }
                    
                    # ç¡®ä¿å›¾ç‰‡ä¿¡æ¯è¢«æ­£ç¡®ä¼ é€’
                    if hasattr(test_product, 'images') and test_product.images:
                        result['images'] = test_product.images
                    
                    return result
            return None
            
    except Exception as e:
        print(f"âŒ GraphQLæ–¹æ³•å‡ºé”™: {e}")
        return None

def scrape_with_enhanced(url, save_file=None):
    """ä½¿ç”¨å¢å¼ºç‰ˆæ–¹æ³•çˆ¬å–"""
    print("ğŸ”„ ä½¿ç”¨å¢å¼ºç‰ˆçˆ¬å–æ–¹æ³•...")
    
    if not enhanced_scrape_puma:
        print("âŒ å¢å¼ºç‰ˆçˆ¬è™«æ¨¡å—ä¸å¯ç”¨")
        return None
    
    try:
        product = enhanced_scrape_puma(url)
        
        if product and product.get('name'):
            product['scraped_at'] = datetime.now().isoformat()
            product['method'] = 'enhanced'
            
            # å°è¯•è·å–è¯¦ç»†å°ºç ä¿¡æ¯
            print("ğŸ” è·å–è¯¦ç»†å°ºç ä¿¡æ¯...")
            enhanced_sizes = get_enhanced_sizes(url)
            if enhanced_sizes and enhanced_sizes.get('all_sizes'):
                product['sizes'] = enhanced_sizes['all_sizes']
                product['available_sizes'] = enhanced_sizes.get('available_sizes', [])
                product['unavailable_sizes'] = enhanced_sizes.get('unavailable_sizes', [])
                product['size_groups'] = enhanced_sizes.get('size_groups', [])
                product['method'] = 'enhanced_with_sizes'
                print(f"   âœ… æˆåŠŸè·å– {len(enhanced_sizes['all_sizes'])} ä¸ªå°ºç ")
            else:
                print("   âš ï¸ æœªèƒ½è·å–è¯¦ç»†å°ºç ï¼Œä¿æŒåŸæœ‰æ•°æ®")
            
            if save_file:
                with open(save_file, 'w', encoding='utf-8') as f:
                    json.dump(product, f, ensure_ascii=False, indent=2)
                print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {save_file}")
            
            return product
        else:
            print("âŒ å¢å¼ºç‰ˆæ–¹æ³•è·å–æ•°æ®å¤±è´¥")
            return None
            
    except Exception as e:
        print(f"âŒ å¢å¼ºç‰ˆæ–¹æ³•å‡ºé”™: {e}")
        return None

def auto_scrape(url, save_file=None):
    """è‡ªåŠ¨é€‰æ‹©æœ€ä½³çˆ¬å–æ–¹æ³•"""
    print("ğŸ¤– è‡ªåŠ¨é€‰æ‹©æœ€ä½³çˆ¬å–æ–¹æ³•...")
    
    # é¦–å…ˆå°è¯•GraphQLæ–¹æ³•ï¼ˆæœ€å®Œæ•´çš„æ•°æ®ï¼‰
    result = scrape_with_graphql(url, save_file)
    if result and result.get('name'):
        print("âœ… GraphQLæ–¹æ³•æˆåŠŸ")
        return result
    
    # å¦‚æœGraphQLå¤±è´¥ï¼Œå°è¯•å¢å¼ºç‰ˆæ–¹æ³•
    print("ğŸ”„ GraphQLå¤±è´¥ï¼Œå°è¯•å¢å¼ºç‰ˆæ–¹æ³•...")
    result = scrape_with_enhanced(url, save_file)
    if result and result.get('name'):
        print("âœ… å¢å¼ºç‰ˆæ–¹æ³•æˆåŠŸ")
        return result
    
    # å¦‚æœå¢å¼ºç‰ˆå¤±è´¥ï¼Œå°è¯•requestsæ–¹æ³•
    print("ğŸ”„ å¢å¼ºç‰ˆå¤±è´¥ï¼Œå°è¯•requestsæ–¹æ³•...")
    result = scrape_with_requests(url, save_file)
    if result and result.get('name'):
        print("âœ… requestsæ–¹æ³•æˆåŠŸ")
        return result
    
    print("âŒ æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥äº†")
    return None

def display_product_info(product):
    """æ˜¾ç¤ºå•†å“ä¿¡æ¯"""
    if not product:
        print("âŒ æ²¡æœ‰å•†å“ä¿¡æ¯å¯æ˜¾ç¤º")
        return
    
    print("\\n" + "="*70)
    print("ğŸ›ï¸  PUMAå•†å“ä¿¡æ¯çˆ¬å–ç»“æœ")
    print("="*70)
    
    # åŸºæœ¬ä¿¡æ¯
    print(f"ğŸ“¦ å•†å“åç§°: {product.get('name', 'N/A')}")
    print(f"ğŸ·ï¸  å“ç‰Œ: {product.get('brand', 'PUMA')}")
    
    # ä»·æ ¼ä¿¡æ¯
    price = product.get('price', '')
    currency = product.get('currency', 'USD')
    if price:
        print(f"ğŸ’° å½“å‰ä»·æ ¼: {currency} ${price}")
    else:
        print("ğŸ’° å½“å‰ä»·æ ¼: N/A")
    
    original_price = product.get('original_price', '')
    if original_price:
        print(f"ğŸ’¸ åŸä»·: {original_price}")
    
    # å•†å“æè¿°
    description = product.get('description', '')
    if description:
        desc_preview = description[:200] + "..." if len(description) > 200 else description
        print(f"ğŸ“ å•†å“æè¿°: {desc_preview}")
    
    product_id = product.get('product_id', '')
    if product_id:
        print(f"ğŸ†” å•†å“ID: {product_id}")
    
    availability = product.get('availability', '')
    if availability:
        print(f"ğŸ“¦ åº“å­˜çŠ¶æ€: {availability}")
    
    # å°ºç ä¿¡æ¯
    sizes = product.get('sizes', [])
    if sizes:
        size_display = ', '.join(sizes[:15])  # æ˜¾ç¤ºå‰15ä¸ªå°ºç 
        print(f"ğŸ‘Ÿ å¯ç”¨å°ºç  ({len(sizes)}ä¸ª): {size_display}")
        if len(sizes) > 15:
            print(f"    ... è¿˜æœ‰{len(sizes) - 15}ä¸ªå°ºç ")
    
    # å›¾ç‰‡ä¿¡æ¯
    images = product.get('images', [])
    print(f"ğŸ–¼ï¸  å•†å“å›¾ç‰‡: {len(images)}å¼ ")
    
    # äº§å“ç‰¹æ€§
    features = product.get('features', [])
    if features:
        print(f"âœ¨ äº§å“ç‰¹æ€§ ({len(features)}ä¸ª):")
        for i, feature in enumerate(features[:3], 1):
            print(f"   {i}. {feature}")
        if len(features) > 3:
            print(f"   ... è¿˜æœ‰{len(features) - 3}ä¸ªç‰¹æ€§")
    
    # äº§å“è¯¦æƒ…ï¼ˆGraphQLç‰¹æœ‰ï¼‰
    details = product.get('details', [])
    if details:
        print(f"ğŸ“‹ äº§å“è¯¦æƒ… ({len(details)}ä¸ª):")
        for i, detail in enumerate(details[:3], 1):
            print(f"   {i}. {detail}")
        if len(details) > 3:
            print(f"   ... è¿˜æœ‰{len(details) - 3}ä¸ªè¯¦æƒ…")
    
    # ææ–™ç»„æˆï¼ˆGraphQLç‰¹æœ‰ï¼‰
    materials = product.get('material_composition', [])
    if materials:
        print(f"ğŸ§µ ææ–™ç»„æˆ:")
        for material in materials:
            print(f"   â€¢ {material}")
    
    # å°ºç è¯¦æƒ…ï¼ˆGraphQLç‰¹æœ‰ï¼‰
    mens_sizes = product.get('mens_sizes', [])
    womens_sizes = product.get('womens_sizes', [])
    
    if mens_sizes or womens_sizes:
        print(f"ğŸ‘Ÿ è¯¦ç»†å°ºç ä¿¡æ¯:")
        
        if mens_sizes:
            available_mens = [s['label'] for s in mens_sizes if s.get('orderable', True)]
            unavailable_mens = [s['label'] for s in mens_sizes if not s.get('orderable', True)]
            print(f"   ğŸ‘¨ ç”·ç  ({len(available_mens)}ä¸ªå¯ç”¨): {', '.join(available_mens)}")
            if unavailable_mens:
                print(f"   âŒ ç”·ç ç¼ºè´§: {', '.join(unavailable_mens)}")
        
        if womens_sizes:
            available_womens = [s['label'] for s in womens_sizes if s.get('orderable', True)]
            unavailable_womens = [s['label'] for s in womens_sizes if not s.get('orderable', True)]
            print(f"   ğŸ‘© å¥³ç  ({len(available_womens)}ä¸ªå¯ç”¨): {', '.join(available_womens)}")
            if unavailable_womens:
                print(f"   âŒ å¥³ç ç¼ºè´§: {', '.join(unavailable_womens)}")
    
    # çˆ¬å–æ–¹æ³•å’Œæ—¶é—´
    method = product.get('method', 'unknown')
    scraped_at = product.get('scraped_at', '')
    print(f"ğŸ”§ çˆ¬å–æ–¹æ³•: {method}")
    if scraped_at:
        print(f"â° çˆ¬å–æ—¶é—´: {scraped_at}")
    
    print("="*70)
    
    # è¯„åˆ†ä¿¡æ¯
    rating = product.get('rating', '')
    reviews_count = product.get('reviews_count', '')
    if rating:
        print(f"â­ è¯„åˆ†: {rating}")
    if reviews_count:
        print(f"ğŸ’¬ è¯„è®ºæ•°: {reviews_count}")
    
    # å•†å“æè¿°
    description = product.get('description', '')
    if description:
        desc_preview = description[:200] + "..." if len(description) > 200 else description
        print(f"ğŸ“ å•†å“æè¿°: {desc_preview}")
    
    # çˆ¬å–ä¿¡æ¯
    method = product.get('method', 'unknown')
    scraped_at = product.get('scraped_at', '')
    print(f"\\nğŸ”§ çˆ¬å–æ–¹æ³•: {method}")
    if scraped_at:
        print(f"â° çˆ¬å–æ—¶é—´: {scraped_at}")
    
    print("="*70)
    
    # æ˜¾ç¤ºå›¾ç‰‡é“¾æ¥
    if images:
        print(f"\\nğŸ–¼ï¸  å•†å“å›¾ç‰‡é“¾æ¥ (å‰5å¼ ):")
        for i, img in enumerate(images[:5], 1):
            print(f"  {i}. {img}")
        if len(images) > 5:
            print(f"  ... è¿˜æœ‰{len(images) - 5}å¼ å›¾ç‰‡")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='Pumaå•†å“ä¿¡æ¯çˆ¬è™« - ä¸“ä¸šç‰ˆ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ä½¿ç”¨ç¤ºä¾‹:
  %(prog)s --url "https://us.puma.com/us/en/pd/product-name/123456"
  %(prog)s --url "..." --method enhanced --output my_product.json
  %(prog)s --url "..." --method auto --verbose
        '''
    )
    
    parser.add_argument('--url', '-u', 
                       default='https://us.puma.com/us/en/pd/evospeed-mid-distance-nitro-elite-3-track--field-distance-spikes/312637?swatch=01',
                       help='å•†å“é¡µé¢URL (é»˜è®¤ä¸ºç¤ºä¾‹å•†å“)')
    
    parser.add_argument('--method', '-m', 
                       choices=['requests', 'enhanced', 'graphql', 'auto'], 
                       default='auto',
                       help='çˆ¬å–æ–¹æ³• (é»˜è®¤: auto, graphqlè·å–æœ€å®Œæ•´æ•°æ®)')
    
    parser.add_argument('--output', '-o', 
                       help='è¾“å‡ºJSONæ–‡ä»¶å (å¯é€‰)')
    
    parser.add_argument('--verbose', '-v', 
                       action='store_true',
                       help='è¯¦ç»†è¾“å‡ºæ¨¡å¼')
    
    parser.add_argument('--validate', 
                       action='store_true',
                       help='éªŒè¯URLæœ‰æ•ˆæ€§')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    setup_logging(args.verbose)
    
    # æ˜¾ç¤ºå¯åŠ¨ä¿¡æ¯
    print("ğŸš€ å¯åŠ¨Pumaå•†å“ä¿¡æ¯çˆ¬è™«...")
    print(f"ğŸ”— ç›®æ ‡URL: {args.url}")
    print(f"âš™ï¸  çˆ¬å–æ–¹æ³•: {args.method}")
    if args.output:
        print(f"ğŸ’¾ è¾“å‡ºæ–‡ä»¶: {args.output}")
    
    # éªŒè¯URL
    if args.validate and not validate_url(args.url):
        response = input("æ˜¯å¦ç»§ç»­? (y/N): ")
        if response.lower() != 'y':
            print("âŒ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
            sys.exit(1)
    
    # æ‰§è¡Œçˆ¬å–
    try:
        if args.method == 'requests':
            result = scrape_with_requests(args.url, args.output)
        elif args.method == 'enhanced':
            result = scrape_with_enhanced(args.url, args.output)
        elif args.method == 'graphql':
            result = scrape_with_graphql(args.url, args.output)
        else:  # auto
            result = auto_scrape(args.url, args.output)
        
        # æ˜¾ç¤ºç»“æœ
        display_product_info(result)
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šè¾“å‡ºæ–‡ä»¶ä½†è·å–äº†æ•°æ®ï¼Œè¯¢é—®æ˜¯å¦ä¿å­˜
        if result and not args.output:
            response = input("\\nğŸ’¾ æ˜¯å¦ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶? (y/N): ")
            if response.lower() == 'y':
                filename = f"puma_product_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                with open(filename, 'w', encoding='utf-8') as f:
                    json.dump(result, f, ensure_ascii=False, indent=2)
                print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {filename}")
        
        return result
        
    except KeyboardInterrupt:
        print("\\nâŒ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()