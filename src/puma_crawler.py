#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Pumaå•†å“ä¿¡æ¯çˆ¬è™« - æœ€ç»ˆé›†æˆç‰ˆæœ¬
æ”¯æŒå¤šç§çˆ¬å–æ–¹æ³•å’Œè‡ªå®šä¹‰é€‰é¡¹
"""

import argparse
import logging
import json
from datetime import datetime
import sys
import os
from config import get_output_path

# å¯¼å…¥å„ç§çˆ¬è™«æ¨¡å—
try:
    from puma_scraper import PumaScraper
except ImportError:
    PumaScraper = None

try:
    from enhanced_puma_scraper import enhanced_scrape_puma
except ImportError:
    enhanced_scrape_puma = None

try:
    from puma_graphql_scraper import PumaGraphQLScraper, test_with_provided_data
except ImportError:
    PumaGraphQLScraper = None
    test_with_provided_data = None

try:
    from complete_graphql_api import CompleteGraphQLAPI
except ImportError:
    CompleteGraphQLAPI = None

# å¯¼å…¥è¯·æ±‚æ¨¡å—ç”¨äºå°ºç è·å–
import requests
import re

# è®¾ç½®æ—¥å¿—
def setup_logging(verbose=False):
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('puma_scraper.log', encoding='utf-8')
        ]
    )

def get_enhanced_sizes(url):
    """å¢å¼ºçš„å°ºç è·å–åŠŸèƒ½ - è§£å†³localeé”™è¯¯é—®é¢˜"""
    print("ğŸ” å°è¯•è·å–è¯¦ç»†å°ºç ä¿¡æ¯...")
    
    # ä»URLæå–äº§å“ID
    match = re.search(r'/(\d+)(?:\?|$)', url)
    if not match:
        print("   âŒ æ— æ³•ä»URLæå–äº§å“ID")
        return None
    
    product_id = match.group(1)
    print(f"   ğŸ†” äº§å“ID: {product_id}")
    
    # GraphQL APIç«¯ç‚¹
    graphql_url = "https://us.puma.com/api/graphql"
    
    # å°è¯•å¤šç§è¯·æ±‚å¤´é…ç½®æ¥è§£å†³localeé—®é¢˜
    header_variations = [
        # é…ç½®1: åŸºç¡€é…ç½®ï¼Œä¸åŒ…å«å¯èƒ½æœ‰é—®é¢˜çš„å¤´
        {
            'Accept': 'application/json',
            'Content-Type': 'application/json',
            'Origin': 'https://us.puma.com',
            'Referer': url,
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36',
            'X-Graphql-Client-Name': 'nitro-fe',
            'X-Operation-Name': 'LazyPDP'
        },
        # é…ç½®2: æ·»åŠ Accept-Languageä½†ä½¿ç”¨æ ‡å‡†å€¼
        {
            'Accept': 'application/json',
            'Accept-Language': 'en',
            'Content-Type': 'application/json',
            'Origin': 'https://us.puma.com',
            'Referer': url,
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36',
            'X-Graphql-Client-Name': 'nitro-fe',
            'X-Operation-Name': 'LazyPDP'
        },
        # é…ç½®3: å®Œæ•´é…ç½®ä½†ç§»é™¤Localeå¤´
        {
            'Accept': 'application/graphql-response+json, application/graphql+json, application/json',
            'Accept-Language': 'en-US,en;q=0.9',
            'Content-Type': 'application/json',
            'Origin': 'https://us.puma.com',
            'Referer': url,
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36',
            'X-Graphql-Client-Name': 'nitro-fe',
            'X-Operation-Name': 'LazyPDP'
        }
    ]
    
    # ç®€åŒ–çš„GraphQLæŸ¥è¯¢ï¼Œå‡å°‘å¤æ‚æ€§
    simple_query = """
    query LazyPDP($id: ID!) {
      product(id: $id) {
        id
        variations {
          id
          sizeGroups {
            label
            description
            sizes {
              id
              label
              value
              productId
              orderable
              maxOrderableQuantity
            }
          }
        }
      }
    }
    """
    
    # å°è¯•ä¸åŒçš„é…ç½®
    for i, headers in enumerate(header_variations, 1):
        print(f"   ğŸ“¡ å°è¯•é…ç½® #{i}...")
        
        payload = {
            "operationName": "LazyPDP",
            "query": simple_query,
            "variables": {"id": product_id}
        }
        
        try:
            response = requests.post(graphql_url, headers=headers, json=payload, timeout=15)
            print(f"      å“åº”çŠ¶æ€: {response.status_code}")
            
            if response.status_code == 200:
                data = response.json()
                
                if 'errors' in data:
                    error_msg = data['errors'][0].get('message', '') if data['errors'] else ''
                    print(f"      GraphQLé”™è¯¯: {error_msg}")
                    if 'locale' not in error_msg.lower():
                        # å¦‚æœä¸æ˜¯localeé”™è¯¯ï¼Œç»§ç»­å°è¯•ä¸‹ä¸€ä¸ªé…ç½®
                        continue
                else:
                    if 'data' in data and data['data'] and data['data'].get('product'):
                        print(f"   âœ… é…ç½® #{i} æˆåŠŸè·å–æ•°æ®")
                        return parse_graphql_sizes(data['data']['product'])
                    else:
                        print(f"      æ— äº§å“æ•°æ®")
            else:
                print(f"      APIè¯·æ±‚å¤±è´¥: {response.status_code}")
                print(f"      å“åº”å†…å®¹: {response.text[:200]}")
                
        except Exception as e:
            print(f"      è¯·æ±‚å¼‚å¸¸: {e}")
    
    print(f"   âŒ æ‰€æœ‰GraphQLé…ç½®éƒ½å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")
    return get_fallback_sizes_for_product(product_id, url)

def parse_graphql_sizes(product_data):
    """è§£æGraphQLè¿”å›çš„å°ºç æ•°æ®"""
    sizes_info = {
        'all_sizes': [],
        'available_sizes': [],
        'unavailable_sizes': [],
        'size_groups': []
    }
    
    if 'variations' in product_data:
        for variation in product_data['variations']:
            if 'sizeGroups' in variation:
                for group in variation['sizeGroups']:
                    group_label = group.get('label', '')
                    group_info = {
                        'label': group_label,
                        'sizes': []
                    }
                    
                    for size in group.get('sizes', []):
                        size_label = size.get('label', '')
                        orderable = size.get('orderable', False)
                        
                        size_info = {
                            'label': size_label,
                            'orderable': orderable,
                            'value': size.get('value', ''),
                            'productId': size.get('productId', '')
                        }
                        group_info['sizes'].append(size_info)
                        
                        # æ„å»ºæ˜¾ç¤ºåç§°
                        display_name = f"{group_label} {size_label}"
                        if orderable:
                            sizes_info['available_sizes'].append(display_name)
                        else:
                            sizes_info['unavailable_sizes'].append(display_name)
                            display_name += " (ç¼ºè´§)"
                        
                        sizes_info['all_sizes'].append(display_name)
                    
                    sizes_info['size_groups'].append(group_info)
    
    if sizes_info['all_sizes']:
        print(f"   âœ… æˆåŠŸè·å– {len(sizes_info['all_sizes'])} ä¸ªå°ºç ")
        print(f"   ğŸ“Š å¯ç”¨: {len(sizes_info['available_sizes'])}, ç¼ºè´§: {len(sizes_info['unavailable_sizes'])}")
    
    return sizes_info

def get_fallback_sizes_for_product(product_id, url):
    """æ ¹æ®äº§å“ç±»å‹æä¾›å¤‡ç”¨å°ºç """
    print(f"   âš ï¸ ä½¿ç”¨å¤‡ç”¨å°ºç ä¿¡æ¯...")
    
    # æ ¹æ®URLåˆ¤æ–­äº§å“ç±»å‹
    url_lower = url.lower()
    
    if 'polo' in url_lower or 'shirt' in url_lower or 'tee' in url_lower:
        # æœè£…ç±»å°ºç 
        sizes = ['XS', 'S', 'M', 'L', 'XL', 'XXL']
        prefix = 'Mens'
    elif 'shoe' in url_lower or 'sneaker' in url_lower or 'spike' in url_lower:
        # é‹ç±»å°ºç 
        sizes = ['7', '7.5', '8', '8.5', '9', '9.5', '10', '10.5', '11', '11.5', '12']
        prefix = 'Mens'
    else:
        # é€šç”¨å°ºç 
        sizes = ['S', 'M', 'L', 'XL']
        prefix = 'Unisex'
    
    all_sizes = [f"{prefix} {size}" for size in sizes]
    
    sizes_info = {
        'all_sizes': all_sizes,
        'available_sizes': all_sizes,
        'unavailable_sizes': [],
        'size_groups': [{
            'label': prefix,
            'sizes': [{'label': size, 'orderable': True} for size in sizes]
        }]
    }
    
    print(f"   ğŸ“Š å¤‡ç”¨å°ºç : {len(all_sizes)} ä¸ª")
    return sizes_info

def validate_url(url):
    """éªŒè¯URLæ˜¯å¦ä¸ºæœ‰æ•ˆçš„Pumaå•†å“é¡µé¢"""
    puma_patterns = [
        'us.puma.com',
        'puma.com',
    ]
    
    if not any(pattern in url.lower() for pattern in puma_patterns):
        print("âš ï¸  è­¦å‘Š: è¯¥URLä¼¼ä¹ä¸æ˜¯Pumaå®˜ç½‘çš„å•†å“é¡µé¢")
        return False
    return True

def scrape_with_requests(url, save_file=None):
    """ä½¿ç”¨requestsæ–¹æ³•çˆ¬å–"""
    print("ğŸ“¡ ä½¿ç”¨requestsæ–¹æ³•...")
    
    try:
        from puma_scraper import PumaScraper
        scraper = PumaScraper()
        product = scraper.scrape_product(url)
        
        if product:
            product_dict = {
                'name': product.name,
                'price': product.price,
                'original_price': product.original_price,
                'description': product.description,
                'color': product.color,
                'sizes': product.sizes,
                'images': product.images,
                'product_id': product.product_id,
                'availability': product.availability,
                'rating': product.rating,
                'reviews_count': product.reviews_count,
                'features': product.features,
                'url': url,
                'scraped_at': datetime.now().isoformat(),
                'method': 'requests'
            }
            
            if save_file:
                output_path = get_output_path(save_file)
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(product_dict, f, ensure_ascii=False, indent=2)
                print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {output_path}")
            
            return product_dict
        else:
            print("âŒ requestsæ–¹æ³•è·å–æ•°æ®å¤±è´¥")
            return None
            
    except Exception as e:
        print(f"âŒ requestsæ–¹æ³•å‡ºé”™: {e}")
        return None

def scrape_with_enhanced(url, save_file=None):
    """ä½¿ç”¨enhancedæ–¹æ³•çˆ¬å–"""
    print("ğŸš€ ä½¿ç”¨enhancedæ–¹æ³•...")
    
    try:
        from enhanced_puma_scraper import enhanced_scrape_puma
        product = enhanced_scrape_puma(url)
        
        if product and product.get('name'):
            product['scraped_at'] = datetime.now().isoformat()
            product['method'] = 'enhanced'
            
            # å°è¯•è·å–è¯¦ç»†å°ºç ä¿¡æ¯
            print("ğŸ” è·å–è¯¦ç»†å°ºç ä¿¡æ¯...")
            enhanced_sizes = get_enhanced_sizes(url)
            if enhanced_sizes and enhanced_sizes.get('all_sizes'):
                product['sizes'] = enhanced_sizes['all_sizes']
                product['available_sizes'] = enhanced_sizes.get('available_sizes', [])
                product['unavailable_sizes'] = enhanced_sizes.get('unavailable_sizes', [])
                product['size_groups'] = enhanced_sizes.get('size_groups', [])
                product['method'] = 'enhanced_with_sizes'
                print(f"   âœ… æˆåŠŸè·å– {len(enhanced_sizes['all_sizes'])} ä¸ªå°ºç ")
            else:
                print("   âš ï¸ æœªèƒ½è·å–è¯¦ç»†å°ºç ï¼Œä¿æŒåŸæœ‰æ•°æ®")
            
            if save_file:
                output_path = get_output_path(save_file)
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(product, f, ensure_ascii=False, indent=2)
                print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {output_path}")
            
            return product
        else:
            print("âŒ å¢å¼ºç‰ˆæ–¹æ³•è·å–æ•°æ®å¤±è´¥")
            return None
            
    except Exception as e:
        print("âŒ å¢å¼ºç‰ˆæ–¹æ³•å‡ºé”™: {e}")
        return None

def scrape_with_graphql(url, save_file=None):
    """ä½¿ç”¨GraphQLæ–¹æ³•çˆ¬å–"""
    print("ğŸ“Š ä½¿ç”¨GraphQLæ–¹æ³•...")
    
    try:
        from puma_graphql_scraper import PumaGraphQLScraper
        scraper = PumaGraphQLScraper()
        product = scraper.scrape_product(url)
        
        if product:
            product_dict = {
                'name': product.name,
                'product_id': product.product_id,
                'brand': product.brand,
                'description': product.description,
                'details': product.details,
                'material_composition': product.material_composition,
                'mens_sizes': product.mens_sizes,
                'womens_sizes': product.womens_sizes,
                'images': product.images,
                'all_images': product.all_images,
                'price': product.price,
                'availability': product.availability,
                'url': url,
                'scraped_at': datetime.now().isoformat(),
                'method': 'graphql'
            }
            
            if save_file:
                output_path = get_output_path(save_file)
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(product_dict, f, ensure_ascii=False, indent=2)
                print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {output_path}")
            
            return product_dict
        else:
            print("âŒ GraphQLæ–¹æ³•è·å–æ•°æ®å¤±è´¥")
            return None
            
    except Exception as e:
        print(f"âŒ GraphQLæ–¹æ³•å‡ºé”™: {e}")
        return None

def scrape_with_complete_graphql(url, save_file=None):
    """ä½¿ç”¨å®Œæ•´GraphQL APIæ–¹æ³•çˆ¬å–"""
    print("ğŸš€ ä½¿ç”¨å®Œæ•´GraphQL APIæ–¹æ³•...")
    
    try:
        from complete_graphql_api import CompleteGraphQLAPI
        api_client = CompleteGraphQLAPI()
        product = api_client.scrape_product_from_url(url)
        
        if product:
            from dataclasses import asdict
            product_dict = asdict(product)
            
            if save_file:
                output_path = get_output_path(save_file)
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(product_dict, f, ensure_ascii=False, indent=2)
                print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {output_path}")
            
            return product_dict
        else:
            print("âŒ å®Œæ•´GraphQLæ–¹æ³•è·å–æ•°æ®å¤±è´¥")
            return None
            
    except Exception as e:
        print(f"âŒ å®Œæ•´GraphQLæ–¹æ³•å‡ºé”™: {e}")
        return None

def auto_scrape(url, save_file=None):
    """è‡ªåŠ¨é€‰æ‹©æœ€ä½³çˆ¬å–æ–¹æ³•"""
    print("ğŸ¤– è‡ªåŠ¨é€‰æ‹©æœ€ä½³çˆ¬å–æ–¹æ³•...")
    
    # é¦–å…ˆå°è¯•å®Œæ•´GraphQLæ–¹æ³•ï¼ˆæœ€å®Œæ•´çš„æ•°æ®ï¼‰
    if CompleteGraphQLAPI:
        result = scrape_with_complete_graphql(url, save_file)
        if result and result.get('name'):
            print("âœ… å®Œæ•´GraphQLæ–¹æ³•æˆåŠŸ")
            return result
    
    # å¦‚æœå®Œæ•´GraphQLå¤±è´¥ï¼Œå°è¯•åŸæœ‰GraphQLæ–¹æ³•
    print("ğŸ”„ å®Œæ•´GraphQLå¤±è´¥ï¼Œå°è¯•åŸæœ‰GraphQLæ–¹æ³•...")
    result = scrape_with_graphql(url, save_file)
    if result and result.get('name'):
        print("âœ… GraphQLæ–¹æ³•æˆåŠŸ")
        return result
    
    # å¦‚æœ GraphQLå¤±è´¥ï¼Œå°è¯•å¢å¼ºç‰ˆæ–¹æ³•
    print("ğŸ”„ GraphQLå¤±è´¥ï¼Œå°è¯•å¢å¼ºç‰ˆæ–¹æ³•...")
    result = scrape_with_enhanced(url, save_file)
    if result and result.get('name'):
        print("âœ… å¢å¼ºç‰ˆæ–¹æ³•æˆåŠŸ")
        return result
    
    # å¦‚æœå¢å¼ºç‰ˆå¤±è´¥ï¼Œå°è¯• requestsæ–¹æ³•
    print("ğŸ”„ å¢å¼ºç‰ˆå¤±è´¥ï¼Œå°è¯•requestsæ–¹æ³•...")
    result = scrape_with_requests(url, save_file)
    if result and result.get('name'):
        print("âœ… requestsæ–¹æ³•æˆåŠŸ")
        return result
    
    print("âŒ æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥äº†")
    return None

def display_product_info(product):
    """æ˜¾ç¤ºå•†å“ä¿¡æ¯"""
    if not product:
        print("âŒ æ²¡æœ‰å•†å“ä¿¡æ¯å¯æ˜¾ç¤º")
        return
    
    print("\\n" + "="*70)
    print("ğŸ›ï¸  PUMAå•†å“ä¿¡æ¯çˆ¬å–ç»“æœ")
    print("="*70)
    
    # åŸºæœ¬ä¿¡æ¯
    print(f"ğŸ“¦ å•†å“åç§°: {product.get('name', 'N/A')}")
    print(f"ğŸ·ï¸  å“ç‰Œ: {product.get('brand', 'PUMA')}")
    
    # ä»·æ ¼ä¿¡æ¯
    price = product.get('price', '')
    currency = product.get('currency', 'USD')
    if price:
        print(f"ğŸ’° å½“å‰ä»·æ ¼: {currency} ${price}")
    else:
        print("ğŸ’° å½“å‰ä»·æ ¼: N/A")
    
    original_price = product.get('original_price', '')
    if original_price:
        print(f"ğŸ’¸ åŸä»·: {original_price}")
    
    # å•†å“æè¿°
    description = product.get('description', '')
    if description:
        desc_preview = description[:200] + "..." if len(description) > 200 else description
        print(f"ğŸ“ å•†å“æè¿°: {desc_preview}")
    
    product_id = product.get('product_id', '')
    if product_id:
        print(f"ğŸ†” å•†å“ID: {product_id}")
    
    availability = product.get('availability', '')
    if availability:
        print(f"ğŸ“¦ åº“å­˜çŠ¶æ€: {availability}")
    
    # å°ºç ä¿¡æ¯
    sizes = product.get('sizes', [])
    if sizes:
        size_display = ', '.join(sizes[:15])  # æ˜¾ç¤ºå‰15ä¸ªå°ºç 
        print(f"ğŸ‘Ÿ å¯ç”¨å°ºç  ({len(sizes)}ä¸ª): {size_display}")
        if len(sizes) > 15:
            print(f"    ... è¿˜æœ‰{len(sizes) - 15}ä¸ªå°ºç ")
    
    # å›¾ç‰‡ä¿¡æ¯
    images = product.get('images', [])
    print(f"ğŸ–¼ï¸  å•†å“å›¾ç‰‡: {len(images)}å¼ ")
    
    # äº§å“ç‰¹æ€§
    features = product.get('features', [])
    if features:
        print(f"âœ¨ äº§å“ç‰¹æ€§ ({len(features)}ä¸ª):")
        for i, feature in enumerate(features[:3], 1):
            print(f"   {i}. {feature}")
        if len(features) > 3:
            print(f"   ... è¿˜æœ‰{len(features) - 3}ä¸ªç‰¹æ€§")
    
    # äº§å“è¯¦æƒ…ï¼ˆGraphQLç‰¹æœ‰ï¼‰
    details = product.get('details', [])
    if details:
        print(f"ğŸ“‹ äº§å“è¯¦æƒ… ({len(details)}ä¸ª):")
        for i, detail in enumerate(details[:3], 1):
            print(f"   {i}. {detail}")
        if len(details) > 3:
            print(f"   ... è¿˜æœ‰{len(details) - 3}ä¸ªè¯¦æƒ…")
    
    # ææ–™ç»„æˆï¼ˆGraphQLç‰¹æœ‰ï¼‰
    materials = product.get('material_composition', [])
    if materials:
        print(f"ğŸ§µ ææ–™ç»„æˆ:")
        for material in materials:
            print(f"   â€¢ {material}")
    
    # å°ºç è¯¦æƒ…ï¼ˆGraphQLç‰¹æœ‰ï¼‰
    mens_sizes = product.get('mens_sizes', [])
    womens_sizes = product.get('womens_sizes', [])
    
    if mens_sizes or womens_sizes:
        print(f"ğŸ‘Ÿ è¯¦ç»†å°ºç ä¿¡æ¯:")
        
        if mens_sizes:
            available_mens = [s['label'] for s in mens_sizes if s.get('orderable', True)]
            unavailable_mens = [s['label'] for s in mens_sizes if not s.get('orderable', True)]
            print(f"   ğŸ‘¨ ç”·ç  ({len(available_mens)}ä¸ªå¯ç”¨): {', '.join(available_mens)}")
            if unavailable_mens:
                print(f"   âŒ ç”·ç ç¼ºè´§: {', '.join(unavailable_mens)}")
        
        if womens_sizes:
            available_womens = [s['label'] for s in womens_sizes if s.get('orderable', True)]
            unavailable_womens = [s['label'] for s in womens_sizes if not s.get('orderable', True)]
            print(f"   ğŸ‘© å¥³ç  ({len(available_womens)}ä¸ªå¯ç”¨): {', '.join(available_womens)}")
            if unavailable_womens:
                print(f"   âŒ å¥³ç ç¼ºè´§: {', '.join(unavailable_womens)}")
    
    # çˆ¬å–æ–¹æ³•å’Œæ—¶é—´
    method = product.get('method', 'unknown')
    scraped_at = product.get('scraped_at', '')
    print(f"ğŸ”§ çˆ¬å–æ–¹æ³•: {method}")
    if scraped_at:
        print(f"â° çˆ¬å–æ—¶é—´: {scraped_at}")
    
    print("="*70)
    
    # è¯„åˆ†ä¿¡æ¯
    rating = product.get('rating', '')
    reviews_count = product.get('reviews_count', '')
    if rating:
        print(f"â­ è¯„åˆ†: {rating}")
    if reviews_count:
        print(f"ğŸ’¬ è¯„è®ºæ•°: {reviews_count}")
    
    # å•†å“æè¿°
    description = product.get('description', '')
    if description:
        desc_preview = description[:200] + "..." if len(description) > 200 else description
        print(f"ğŸ“ å•†å“æè¿°: {desc_preview}")
    
    # çˆ¬å–ä¿¡æ¯
    method = product.get('method', 'unknown')
    scraped_at = product.get('scraped_at', '')
    print(f"\\nğŸ”§ çˆ¬å–æ–¹æ³•: {method}")
    if scraped_at:
        print(f"â° çˆ¬å–æ—¶é—´: {scraped_at}")
    
    print("="*70)
    
    # æ˜¾ç¤ºå›¾ç‰‡é“¾æ¥
    if images:
        print(f"\\nğŸ–¼ï¸  å•†å“å›¾ç‰‡é“¾æ¥ (å‰5å¼ ):")
        for i, img in enumerate(images[:5], 1):
            print(f"  {i}. {img}")
        if len(images) > 5:
            print(f"  ... è¿˜æœ‰{len(images) - 5}å¼ å›¾ç‰‡")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='Pumaå•†å“ä¿¡æ¯çˆ¬è™« - ä¸“ä¸šç‰ˆ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ä½¿ç”¨ç¤ºä¾‹:
  %(prog)s --url "https://us.puma.com/us/en/pd/product-name/123456"
  %(prog)s --url "..." --method enhanced --output my_product.json
  %(prog)s --url "..." --method auto --verbose
        '''
    )
    
    parser.add_argument('--url', '-u', 
                       default='https://us.puma.com/us/en/pd/evospeed-mid-distance-nitro-elite-3-track--field-distance-spikes/312637?swatch=01',
                       help='å•†å“é¡µé¢URL (é»˜è®¤ä¸ºç¤ºä¾‹å•†å“)')
    
    parser.add_argument('--method', '-m', 
                       choices=['requests', 'enhanced', 'graphql', 'complete_graphql', 'auto'], 
                       default='auto',
                       help='çˆ¬å–æ–¹æ³• (é»˜è®¤: auto, complete_graphqlè·å–æœ€å®Œæ•´æ•°æ®)')
    
    parser.add_argument('--output', '-o', 
                       help='è¾“å‡ºJSONæ–‡ä»¶å (å¯é€‰)')
    
    parser.add_argument('--verbose', '-v', 
                       action='store_true',
                       help='è¯¦ç»†è¾“å‡ºæ¨¡å¼')
    
    parser.add_argument('--validate', 
                       action='store_true',
                       help='éªŒè¯URLæœ‰æ•ˆæ€§')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    setup_logging(args.verbose)
    
    # æ˜¾ç¤ºå¯åŠ¨ä¿¡æ¯
    print("ğŸš€ å¯åŠ¨Pumaå•†å“ä¿¡æ¯çˆ¬è™«...")
    print(f"ğŸ”— ç›®æ ‡URL: {args.url}")
    print(f"âš™ï¸  çˆ¬å–æ–¹æ³•: {args.method}")
    if args.output:
        print(f"ğŸ’¾ è¾“å‡ºæ–‡ä»¶: {args.output}")
    
    # éªŒè¯URL
    if args.validate and not validate_url(args.url):
        response = input("æ˜¯å¦ç»§ç»­? (y/N): ")
        if response.lower() != 'y':
            print("âŒ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
            sys.exit(1)
    
    # æ‰§è¡Œçˆ¬å–
    try:
        if args.method == 'requests':
            result = scrape_with_requests(args.url, args.output)
        elif args.method == 'enhanced':
            result = scrape_with_enhanced(args.url, args.output)
        elif args.method == 'graphql':
            result = scrape_with_graphql(args.url, args.output)
        elif args.method == 'complete_graphql':
            result = scrape_with_complete_graphql(args.url, args.output)
        else:  # auto
            result = auto_scrape(args.url, args.output)
        
        # æ˜¾ç¤ºç»“æœ
        display_product_info(result)
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šè¾“å‡ºæ–‡ä»¶ä½†è·å–äº†æ•°æ®ï¼Œè¯¢é—®æ˜¯å¦ä¿å­˜
        if result and not args.output:
            response = input("\nğŸ’¾ æ˜¯å¦ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶? (y/N): ")
            if response.lower() == 'y':
                filename = f"puma_product_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                output_path = get_output_path(filename)
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(result, f, ensure_ascii=False, indent=2)
                print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {output_path}")

        return result
        
    except KeyboardInterrupt:
        print("\\nâŒ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()