#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Pumaå•†å“çˆ¬è™« - URLæŠ“å–ç¤ºä¾‹
å±•ç¤ºå¦‚ä½•æŠ“å–æŒ‡å®šURLçš„å•†å“ä¿¡æ¯
"""

import subprocess
import sys
import json
from datetime import datetime

class PumaURLScraper:
    """Puma URLæŠ“å–å™¨"""
    
    def __init__(self):
        self.base_command = [sys.executable, "puma_crawler.py"]
    
    def scrape_url(self, url, method="auto", output_file=None, verbose=False):
        """
        æŠ“å–æŒ‡å®šURLçš„å•†å“ä¿¡æ¯
        
        Args:
            url (str): å•†å“é¡µé¢URL
            method (str): çˆ¬å–æ–¹æ³• (requests/enhanced/graphql/auto)
            output_file (str): è¾“å‡ºæ–‡ä»¶å (å¯é€‰)
            verbose (bool): æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        
        Returns:
            dict: å•†å“ä¿¡æ¯
        """
        
        print(f"ğŸš€ å¼€å§‹æŠ“å–å•†å“ä¿¡æ¯...")
        print(f"ğŸ”— ç›®æ ‡URL: {url}")
        print(f"âš™ï¸  ä½¿ç”¨æ–¹æ³•: {method}")
        
        # æ„å»ºå‘½ä»¤
        cmd = self.base_command + ["--url", url, "--method", method]
        
        if output_file:
            cmd.extend(["--output", output_file])
            print(f"ğŸ’¾ è¾“å‡ºæ–‡ä»¶: {output_file}")
        
        if verbose:
            cmd.append("--verbose")
        
        try:
            # æ‰§è¡Œçˆ¬å–å‘½ä»¤
            result = subprocess.run(cmd, capture_output=True, text=True, encoding='utf-8')
            
            if result.returncode == 0:
                print("âœ… çˆ¬å–æˆåŠŸï¼")
                
                # å¦‚æœæœ‰è¾“å‡ºæ–‡ä»¶ï¼Œè¯»å–ç»“æœ
                if output_file:
                    try:
                        with open(output_file, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                        return data
                    except Exception as e:
                        print(f"âš ï¸  è¯»å–è¾“å‡ºæ–‡ä»¶å¤±è´¥: {e}")
                        return None
                else:
                    print("â„¹ï¸  ç»“æœå·²æ˜¾ç¤ºåœ¨å‘½ä»¤è¡Œä¸­")
                    return True
            else:
                print(f"âŒ çˆ¬å–å¤±è´¥: {result.stderr}")
                return None
                
        except Exception as e:
            print(f"âŒ æ‰§è¡Œå‡ºé”™: {e}")
            return None
    
    def batch_scrape_urls(self, urls, method="auto", output_dir="batch_results"):
        """
        æ‰¹é‡æŠ“å–å¤šä¸ªURL
        
        Args:
            urls (list): URLåˆ—è¡¨
            method (str): çˆ¬å–æ–¹æ³•
            output_dir (str): è¾“å‡ºç›®å½•
        """
        
        import os
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        results = []
        
        for i, url in enumerate(urls, 1):
            print(f"\nğŸ“¦ æ‰¹é‡æŠ“å– {i}/{len(urls)}")
            
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"{output_dir}/product_{i}_{timestamp}.json"
            
            # æŠ“å–å•†å“ä¿¡æ¯
            result = self.scrape_url(url, method, output_file, verbose=False)
            
            if result:
                results.append({
                    'url': url,
                    'output_file': output_file,
                    'success': True
                })
                print(f"âœ… ç¬¬{i}ä¸ªå•†å“æŠ“å–æˆåŠŸ")
            else:
                results.append({
                    'url': url,
                    'output_file': None,
                    'success': False
                })
                print(f"âŒ ç¬¬{i}ä¸ªå•†å“æŠ“å–å¤±è´¥")
        
        # ä¿å­˜æ‰¹é‡ç»“æœæ‘˜è¦
        summary_file = f"{output_dir}/batch_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump({
                'total_urls': len(urls),
                'successful': len([r for r in results if r['success']]),
                'failed': len([r for r in results if not r['success']]),
                'results': results,
                'scraped_at': datetime.now().isoformat()
            }, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“Š æ‰¹é‡æŠ“å–å®Œæˆï¼æ‘˜è¦ä¿å­˜åˆ°: {summary_file}")
        return results

def main():
    """ç¤ºä¾‹ç”¨æ³•"""
    
    scraper = PumaURLScraper()
    
    # ç¤ºä¾‹1: æŠ“å–å•ä¸ªURL
    print("=" * 60)
    print("ç¤ºä¾‹1: æŠ“å–å•ä¸ªURL")
    print("=" * 60)
    
    example_url = "https://us.puma.com/us/en/pd/evospeed-mid-distance-nitro-elite-3-track--field-distance-spikes/312637?swatch=01"
    
    # ä½¿ç”¨ä¸åŒæ–¹æ³•æŠ“å–
    methods = ["enhanced", "graphql", "auto"]
    
    for method in methods:
        print(f"\nğŸ”§ æµ‹è¯• {method} æ–¹æ³•:")
        output_file = f"example_{method}_result.json"
        result = scraper.scrape_url(example_url, method, output_file)
        
        if result:
            print(f"   âœ… {method} æ–¹æ³•æˆåŠŸ")
        else:
            print(f"   âŒ {method} æ–¹æ³•å¤±è´¥")
    
    # ç¤ºä¾‹2: æ‰¹é‡æŠ“å–å¤šä¸ªURL
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹2: æ‰¹é‡æŠ“å–å¤šä¸ªURL")
    print("=" * 60)
    
    # ç¤ºä¾‹URLåˆ—è¡¨ï¼ˆå¯ä»¥æ›¿æ¢ä¸ºçœŸå®çš„ä¸åŒå•†å“URLï¼‰
    example_urls = [
        "https://us.puma.com/us/en/pd/evospeed-mid-distance-nitro-elite-3-track--field-distance-spikes/312637?swatch=01",
        # å¯ä»¥æ·»åŠ æ›´å¤šURL
        # "https://us.puma.com/us/en/pd/another-product/123456",
        # "https://us.puma.com/us/en/pd/third-product/789012",
    ]
    
    if len(example_urls) > 1:
        batch_results = scraper.batch_scrape_urls(example_urls, method="auto")
        print(f"æ‰¹é‡æŠ“å–ç»“æœ: {len(batch_results)} ä¸ªURLå¤„ç†å®Œæˆ")

if __name__ == "__main__":
    main()